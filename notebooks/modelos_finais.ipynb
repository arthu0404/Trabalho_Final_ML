{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44ea453",
   "metadata": {},
   "source": [
    "## **Notebook com os Modelos finais**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3f282",
   "metadata": {},
   "source": [
    "**Autores:**\n",
    "\n",
    "- Arthur Brandão do Nascimento\n",
    "\n",
    "- Caio Ávila Paulo\n",
    "\n",
    "- Matheus Macedo do Nascimento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f14d48",
   "metadata": {},
   "source": [
    "## **Introdução**\n",
    "Este notebook tem por objetivo implementar e comparar diferentes algoritmos de aprendizado de máquina supervisionado de classificação. A partir de características como composição, tamanho e tempo de exposição de diferentes materiais, será prevista sua toxicidade em relação a diferentes tipos de células.\n",
    "\n",
    "Os algoritmos utilizados serão os de k vizinhos mais próximos (knn), árvore de decisão, floresta aleatória, Support Vector Classifier (SVC) e Regressão Logística. O desempenho de cada um deles será estimado por validação cruzada do tipo k-fold e os hiperparâmetros dos modelos serão otimizados com o Optuna. Além disso, um modelo baseline será estabelecido para fins de comparação.\n",
    "\n",
    "O Optuna também será usado para testar a normalização dos dados. A métrica a ser otimizada será a f1-macro, adequada ao target categórico binário: a partir dos atributos tanto do material quanto da célula, será atribuído o rótulo \"tóxico\" ou \"não tóxico\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c83cb8",
   "metadata": {},
   "source": [
    "### **Importando as bibliotecas necessárias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae229090",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.7)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from optuna import create_study\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8224d56",
   "metadata": {},
   "source": [
    "### **Importando o dataset**\n",
    "\n",
    "O dataset escolhido tem, a princípio, 3923 linhas e 17 colunas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91207d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/dataset_nanotoxicologia_combinado.csv\")\n",
    "\n",
    "display(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b2540",
   "metadata": {},
   "source": [
    "### **Definindo as ``FEATURES`` e o ``TARGET``**\n",
    "\n",
    "As features serão separadas entre aquelas que já são valores numéricos (``FEATURES_NUM``) e aquelas que serão convertidas em valores binários (``FEATURES_DUMMY``).\n",
    "\n",
    "Variável alvo (**``TARGET``**):\n",
    "- ``\"Toxicity\"`` - \n",
    "\n",
    "Features Numéricas (**``FEATURES_NUM``**):\n",
    "\n",
    "``\"Core_size\"``, ``\"Hydro_size\"``, ``\"Surface_charge\"``, ``\"Surface_area\"``, ``\"Formation_enthalpy\"``, ``\"Conduction_band\"``, ``\"Valence_band\"``, ``\"Electronegativity\"``, ``\"Exposure_time\"``, ``\"Exposure_dose\"``\n",
    "\n",
    "Features Categóricas (**``FEATURES_DUMMY``**):\n",
    "\n",
    "``\"Material_type\"``, ``\"Assay\"``, ``\"Cell_name\"``, ``\"Cell_species\"``, ``\"Cell_origin\"``, ``\"Cell_type\"``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55751354",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_NUM = [\"Core_size\", \n",
    "                \"Hydro_size\", \n",
    "                \"Surface_charge\", \n",
    "                \"Surface_area\", \n",
    "                \"Formation_enthalpy\", \n",
    "                \"Conduction_band\", \n",
    "                \"Valence_band\", \n",
    "                \"Electronegativity\", \n",
    "                \"Exposure_time\", \n",
    "                \"Exposure_dose\"\n",
    "]\n",
    "\n",
    "FEATURES_DUMMY = [\"Material_type\", \"Assay\", \"Cell_name\", \"Cell_species\", \"Cell_origin\", \"Cell_type\"]\n",
    "\n",
    "TARGET = [\"Toxicity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a1da49",
   "metadata": {},
   "source": [
    "### **Evitando vazamento de dados pelo ``groupby()``**\n",
    "Pode ser que vários dados do dataset sejam iguais em todos os atributos, diferindo (ou não) apenas no target. Dessa forma, alguns desses dados poderiam acabar sendo usado na etapa de treino e outros na fase de teste. Isso faria com que a métrica não refletisse o real desempenho do modelo; afinal, ele já \"conheceria\" alguns dados de teste, o que acarreta uma previsão enviesada.\n",
    "\n",
    "Para evitar esse tipo de vazamento, *antes* do split de treino e teste, é necessário acabar com essa redundância. Isso é feito agrupando todos os dados duplicados em um só: os atributos continuam os mesmos, mas apenas um valor de target é utilizado, a partir de uma estatística dos dados originais. Como o target é categórico, será usada a moda.\n",
    "\n",
    "Importante que os dados duplicados não precisam ser cópias idênticas: se todos os atributos forem muito próximos (apesar de não serem iguais), o vazamento de dados ocorrerá da mesma maneira. Por isso, antes de identificar dados repetidos e agrupá-los, arredonda-se o valor de cada atributo em uma casa adequada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13d84b",
   "metadata": {},
   "source": [
    "#### **Arredondando**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5afe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "casa_arredondamento = {\n",
    "    \"Core_size\": 0, \n",
    "    \"Hydro_size\": 0, \n",
    "    \"Surface_charge\": 0, \n",
    "    \"Surface_area\": 0, \n",
    "    \"Formation_enthalpy\": 0, \n",
    "    \"Conduction_band\": 0,\n",
    "    \"Valence_band\": 0, \n",
    "    \"Electronegativity\": 0, \n",
    "    \"Exposure_time\": 1, \n",
    "    \"Exposure_dose\": 1,\n",
    "}\n",
    "\n",
    "df_round = df.round(casa_arredondamento)\n",
    "\n",
    "df_round.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d7a234",
   "metadata": {},
   "source": [
    "Como nós podemos ver pelo código abaixo esse dataset possui alguns dados duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb964f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicados = df_round[FEATURES_NUM + FEATURES_DUMMY].duplicated().sum()\n",
    "print(f\"Número de linhas duplicadas em df_round: {num_duplicados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56d2ef",
   "metadata": {},
   "source": [
    "#### **Agrupando**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce360a28",
   "metadata": {},
   "source": [
    "O [``agg()``](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html) é um método do pandas usado para aplicar uma ou mais operações de agregação em grupos de dados. Como o target (``\"Toxicity\"``) é categórico, usaremos a moda para decidir o rótulo daquele grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5173552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def calcular_moda(serie):\n",
    "    \"\"\"Calcula a moda de uma série, retornando o primeiro valor se não houver moda única clara\"\"\"\n",
    "    moda = stats.mode(serie)\n",
    "    return moda[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6124a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_round.groupby(FEATURES_NUM + FEATURES_DUMMY, sort=False)\n",
    "\n",
    "agg_dict = {\n",
    "    **{col: \"mean\" for col in FEATURES_NUM},\n",
    "    **{col: calcular_moda for col in FEATURES_DUMMY},\n",
    "    \"Toxicity\": calcular_moda\n",
    "}\n",
    "\n",
    "df_grouped = df_grouped.agg(agg_dict)\n",
    "df_grouped = df_grouped.reset_index(drop=True)\n",
    "\n",
    "print(f\"O shape (linhas x colunas) do df_round é: {df_round.shape}\")\n",
    "print(f\"O shape (linhas x colunas) do df_tratado é: {df_grouped.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicados = df_grouped[FEATURES_NUM + FEATURES_DUMMY].duplicated().sum()\n",
    "print(f\"Número de linhas duplicadas em df_grouped: {num_duplicados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0d7ef",
   "metadata": {},
   "source": [
    "Como podemos ver não há mais valores duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d8933",
   "metadata": {},
   "source": [
    "### **Fazendo a codificação One-Hot**\n",
    "Os algoritmos de aprendizado de máquina utilizados exigem que todos os atributos sejam numéricos. Dessa forma, é necessário transformar os dados qualitativos adequadamente.  O codificador One-Hot transforma uma coluna de dados categóricos em várias colunas, cada qual representando um dos rótulos possíveis; se o dado originalmente tinha aquele rótulo, atribui-se o valor 1, caso contrário preenche-se com 0. Isso será feito com todos os ``\"FEATURES_DUMMY\"``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b40473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False, dtype=np.int32)\n",
    "dummy_encoded = encoder.fit_transform(df_grouped[FEATURES_DUMMY])\n",
    "\n",
    "dummy_columns = encoder.get_feature_names_out(FEATURES_DUMMY)\n",
    "df_dummy = pd.DataFrame(dummy_encoded, columns=dummy_columns, index=df_grouped.index)\n",
    "\n",
    "df_dummy = pd.concat([df_grouped[FEATURES_NUM + TARGET], df_dummy], axis=1)\n",
    "FEATURES_FINAL = FEATURES_NUM + list(dummy_columns)\n",
    "\n",
    "display(df_dummy.shape)\n",
    "display(df_dummy.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9e2c1",
   "metadata": {},
   "source": [
    "### **Definindo os dados de treino e de teste**\n",
    "Com o dataframe devidamente tratado, pode ser feita a divisão dos dados em teste e treino. No caso será utilizado o ``stratify`` pois há um certo desbalanço no dataset, havendo mais dados sobre nanopartículas não tóxicas do que nanopartículas tóxicas, o ``\"stratify\"`` mantém a proporção das classes em ambos os conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAMANHO_TESTE = 0.25\n",
    "SEED = 404\n",
    "\n",
    "valores_target = df_dummy[TARGET].values.ravel()\n",
    "\n",
    "df_treino, df_teste = train_test_split(df_dummy, test_size=TAMANHO_TESTE, random_state=SEED, stratify=valores_target)\n",
    "\n",
    "X_teste = df_teste.reindex(FEATURES_FINAL, axis=1)\n",
    "y_teste = df_teste.reindex(TARGET, axis=1).values.ravel()\n",
    "\n",
    "X_treino = df_treino.reindex(FEATURES_FINAL, axis=1)\n",
    "y_treino = df_treino.reindex(TARGET, axis=1).values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d5eb3",
   "metadata": {},
   "source": [
    "### **Criando os modelos e espaços de busca**\n",
    "\n",
    "Serão criados os seguintes modelos para comparação:\n",
    "\n",
    "- Baseline (DummyClassifier)\n",
    "- k-Nearest Neighbors classifier (KNN)\n",
    "- Árvore de Decisão\n",
    "- Floresta Aleatória\n",
    "- Regressão Logística\n",
    "- Support Vector Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c32d8",
   "metadata": {},
   "source": [
    "#### **Baseline**\n",
    "\n",
    "O baseline estabelece uma referência mínima de desempenho, onde qualquer modelo útil deve superar essa referência.\n",
    "\n",
    "No caso, foi utilizado o ``DummyClassifier(strategy='most_frequent')``, que prevê sempre a moda dos valores de y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_baseline = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "NUM_FOLDS = 10\n",
    "\n",
    "f1_macro_estimativa_dummy = cross_val_score(modelo_baseline, X_treino, y_treino, scoring=\"f1_macro\", cv=NUM_FOLDS).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7684c60",
   "metadata": {},
   "source": [
    "#### **Implementação dos Modelos com o Optuna**\n",
    "O Optuna é um framework de otimização de hiperparâmetros. Ele automatiza o processo de enontrar o conjunto ótimo de hiperparâmetros para um dado modelo, almejando minimizar ou maximizar uma função objetiva específica.\n",
    "\n",
    "No Optuna, os ``Trials`` são as tentativas com diferentes combinações de hiperparâmetros e o ``Study`` é o conjunto de trials para um determinado objetivo.\n",
    "\n",
    "Nesse contexto, a função ``cria_instancia_modelo`` serve para criar uma instância do modelo escolhido, recebendo um trial.\n",
    "Para definir o espaço do dicionário dos parâmetros é usado o ``trial.suggest_*()``.\n",
    "\n",
    "Além disso, foi utilizada a decisão de normalização dos dados como um hiperparâmetro adicional. Para isso foi criado um curto **pipeline** com o ``make_pipeline()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a112195a",
   "metadata": {},
   "source": [
    "#### **Instância K-NN**\n",
    "\n",
    "Baseado no princípio de que exemplos similares tendem a pertencer à mesma classe. Para classificar uma nova amostra, o algoritmo calcula as distâncias entre essa amostra e os pontos do conjunto de treinamento, identifica os K vizinhos mais próximos e realiza uma votação majoritária entre suas classes.\n",
    "\n",
    "Hiperparâmetros Otimizados:\n",
    "\n",
    "- **``n_neighbors``**: número de vizinhos\n",
    "\n",
    "- **``weights``**: uniform, sem pesos, ou distance, em que vizinhos mais próximos tem mais peso\n",
    "\n",
    "- **``p``**: tipo de distância, 1=Manhattan, 2=Euclidiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cria_instancia_knn(trial):\n",
    "    \"\"\"Cria uma instância de um modelo KNN.\"\"\"\n",
    "\n",
    "    parametros = {\n",
    "        \"n_neighbors\": trial.suggest_int(\"num_vizinhos\", 1, 200, log=True),\n",
    "        \"weights\": trial.suggest_categorical(\"pesos\", [\"uniform\", \"distance\"]),\n",
    "        \"p\": trial.suggest_int(\"tipo_distancia\", 1, 2),\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    normalizar = trial.suggest_categorical(\"normalizar\", [True, False])\n",
    "\n",
    "    if normalizar:\n",
    "        tipo_normalizacao = trial.suggest_categorical(\"tipo_norm\", [\"Standard\", \"MinMax\", \"MaxAbs\"])\n",
    "\n",
    "        if tipo_normalizacao == \"Standard\":\n",
    "            normalizador = StandardScaler()\n",
    "        elif tipo_normalizacao == \"MinMax\":\n",
    "            normalizador = MinMaxScaler()\n",
    "        elif tipo_normalizacao == \"MaxAbs\":\n",
    "            normalizador = MaxAbsScaler()\n",
    "            \n",
    "        modelo_knn = make_pipeline(\n",
    "            normalizador,\n",
    "            KNeighborsClassifier(**parametros)\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        modelo_knn = KNeighborsClassifier(**parametros)\n",
    "\n",
    "    return modelo_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd8830c",
   "metadata": {},
   "source": [
    "#### **Instância Árvore de Decisão**\n",
    "\n",
    "O algoritmo da árvore de decisão constrói uma estrutura similar a um fluxograma, onde cada nó interno representa uma decisão baseada em uma feature específica, cada ramo representa o resultado dessa decisão e cada nó folha representa a classe predita. O processo de construção da árvore segue uma estratégia que visa a máxima separação dos nós resultantes em cada divisão, utilizando critérios como entropia ou índice Gini para medir a homogeneidade das classes. A principal vantagem das árvores de decisão em relação às florestas aleatórias reside em sua alta interpretabilidade.\n",
    "\n",
    "Hiperparâmetros Otimizados:\n",
    "\n",
    "- **``max_depth``**: Profundidade máxima da árvore\n",
    "\n",
    "- **``criterion``**: A função usada para medir a qualidade de um _split_ em cada nó\n",
    "\n",
    "- **``min_samples_split``**: Número mínimo de amostras necessárias para dividir um nó\n",
    "\n",
    "- **``min_samples_leaf``**:  Número mínimo de amostras em um nó folha\n",
    "\n",
    "- **``max_features``**: Limita o número de features que o algorítimo utiliza em cada divisão para determinar a melhor features de divisão.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf67324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cria_instancia_dtree(trial):\n",
    "    \"\"\"Cria a instância de um modelo de árvore de decisão\"\"\"\n",
    "\n",
    "    parametros = {\n",
    "        \"max_depth\": trial.suggest_int(\"profundidade\", 2, 600, log=True),\n",
    "        \"criterion\": trial.suggest_categorical(\"critério\", ['entropy', 'log_loss', 'gini']),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_exemplos_split\", 2, 200, log=True),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_exemplos_folha\", 1, 100, log=True),\n",
    "        \"max_features\": trial.suggest_float(\"num_max_features\", 0, 1),\n",
    "        \"random_state\": SEED,\n",
    "    }\n",
    "\n",
    "    normalizar = trial.suggest_categorical(\"normalizar\", [True, False])\n",
    "\n",
    "    if normalizar:\n",
    "        tipo_normalizacao = trial.suggest_categorical(\"tipo_norm\", [\"Standard\", \"MinMax\", \"MaxAbs\"])\n",
    "\n",
    "        if tipo_normalizacao == \"Standard\":\n",
    "            normalizador = StandardScaler()\n",
    "        elif tipo_normalizacao == \"MinMax\":\n",
    "            normalizador = MinMaxScaler()\n",
    "        elif tipo_normalizacao == \"MaxAbs\":\n",
    "            normalizador = MaxAbsScaler()\n",
    "            \n",
    "        modelo_dtree = make_pipeline(\n",
    "            normalizador,\n",
    "            DecisionTreeClassifier(**parametros)\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        modelo_dtree = DecisionTreeClassifier(**parametros)\n",
    "\n",
    "    return modelo_dtree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ce25b6",
   "metadata": {},
   "source": [
    "#### **Instância Floresta Aleatória (RF)**\n",
    "\n",
    "A floresta aleatória é uma técnica que combina o princípio de comitês com aleatorização adicional de features para criar múltiplas árvores de decisão diversas. Cada árvore na floresta é treinada em uma amostra bootstrap, amostragem aleatória com reposição, do conjunto de dados original e, em cada divisão da árvore, apenas um subconjunto aleatório de features é considerado para divisão. Essa aleatorização garante que as árvores individuais sejam diferentes umas das outras, reduzindo a correlação entre seus erros. Durante a predição, todas as árvores votam na classe final, com a maioria decidindo o resultado. Essa abordagem coletiva resulta em um modelo que geralmente supera árvores individuais, embora não seja interpretável e tenha um custo computacional maior.\n",
    "\n",
    "Hiperparâmetros Otimizados:\n",
    "\n",
    "- **``n_estimators``**: Número de árvores utilizadas no modelo\n",
    "\n",
    "- **``criterion``**: A função usada para medir a qualidade de um _split_ em cada nó\n",
    "\n",
    "- **``max_depth``**: Controla profundidade maxíma individual das árvores\n",
    "\n",
    "- **``min_samples_split``**: Número mínimo de amostras necessárias para dividir um nó\n",
    "\n",
    "- **``min_samples_leaf``**: Número mínimo de amostras em um nó folha\n",
    "\n",
    "- **``max_features``**: Limita o número de features que o algorítimo utiliza em cada divisão para determinar a melhor features de divisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cria_instancia_rf(trial):\n",
    "    \"\"\"Cria a instância de um modelo de floresta aleatória.\"\"\"\n",
    "\n",
    "    parametros = {\n",
    "        \"n_estimators\": trial.suggest_int(\"num_arvores\", 2, 1000, log=True),\n",
    "        \"criterion\": trial.suggest_categorical(\"critério\", [\"log_loss\", \"gini\", \"entropy\"]),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 600, log=True),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_exemplos_split\", 2, 200),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_exemplos_folha\", 1, 100),\n",
    "        \"max_features\": trial.suggest_float(\"num_max_atributos\", 0, 1),\n",
    "        \"n_jobs\": -1,\n",
    "        \"bootstrap\": True,\n",
    "        \"random_state\": SEED,\n",
    "    }\n",
    "\n",
    "    normalizar = trial.suggest_categorical(\"normalizar\", [True, False])\n",
    "    if normalizar:\n",
    "        tipo_normalizacao = trial.suggest_categorical(\"tipo_norm\", [\"Standard\", \"MinMax\", \"MaxAbs\"])\n",
    "\n",
    "        if tipo_normalizacao == \"Standard\":\n",
    "            normalizador = StandardScaler()\n",
    "        elif tipo_normalizacao == \"MinMax\":\n",
    "            normalizador = MinMaxScaler()\n",
    "        elif tipo_normalizacao == \"MaxAbs\":\n",
    "            normalizador = MaxAbsScaler()\n",
    "\n",
    "        modelo_rf = make_pipeline(\n",
    "            normalizador,\n",
    "            RandomForestClassifier(**parametros)\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        modelo_rf = RandomForestClassifier(**parametros)\n",
    "\n",
    "    return modelo_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd0e84d",
   "metadata": {},
   "source": [
    "#### **Instância Regressão Logística (LR)**\n",
    "\n",
    "A regressão logística é um algoritmo de classificação que modela a probabilidade de uma amostra pertencer a uma classe específica usando a função logística (sigmoide). O modelo assume uma relação linear entre as features e o logarítimo da chance da probabilidade da classe positiva. A inclusão de termos de regularização (L1, L2 ou Elastic Net) ajuda a prevenir overfitting e a lidar com multicolinearidade entre as features.\n",
    "\n",
    "Hiperparâmetros Otimizados:\n",
    "\n",
    "- **``penalty``**: É o tipo de regularização utilizada \n",
    "\n",
    "- **``C``**: Controla a intesidade da regularização\n",
    "\n",
    "- **``l1_ratio ``**: Balanceamento entre penalidades L1 e L2, utilizada apenas pelo elasticnet\n",
    "\n",
    "Há também um parâmetro chamado `class_weight` que nos auxilia a lidar com variáveis ditas \"não-balanceadas\". Basicamente, esse hiperparâmetro decide se o modelo vai se importar mais com a acurácia no geral ou na identificação correta de _outliers_. Ele ajusta a influência de cada uma dos atributos durante o treinamento, forçando o modelo a prestar mais atenção nos valores menos frequentes em colunas de acordo com o peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de856cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cria_instancia_lr(trial):\n",
    "    \"\"\"Cria a instância de um modelo de Regressão Logística.\"\"\"\n",
    "\n",
    "    parametros = {\n",
    "        \"penalty\": trial.suggest_categorical(\"penalidade\", [\"l1\", \"l2\", \"elasticnet\"]),\n",
    "        \"C\": trial.suggest_float(\"C\", 1e-3, 1e3, log=True),\n",
    "        \"class_weight\": \"balanced\", \n",
    "        \"solver\": \"saga\",\n",
    "        \"max_iter\": 10000,\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": SEED,\n",
    "    }\n",
    "\n",
    "    if parametros[\"penalty\"] == \"elasticnet\":\n",
    "        parametros[\"l1_ratio\"] = trial.suggest_float(\"l1_ratio\", 0.1, 0.9)\n",
    "    \n",
    "    normalizar = trial.suggest_categorical(\"normalizar\", [True, False])\n",
    "\n",
    "    if normalizar:\n",
    "        tipo_normalizacao = trial.suggest_categorical(\"tipo_norm\", [\"Standard\", \"MinMax\", \"MaxAbs\"])\n",
    "\n",
    "        if tipo_normalizacao == \"Standard\":\n",
    "            normalizador = StandardScaler()\n",
    "        elif tipo_normalizacao == \"MinMax\":\n",
    "            normalizador = MinMaxScaler()\n",
    "        elif tipo_normalizacao == \"MaxAbs\":\n",
    "            normalizador = MaxAbsScaler()\n",
    "            \n",
    "        modelo_lr = make_pipeline(\n",
    "            normalizador,\n",
    "            LogisticRegression(**parametros)\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        modelo_lr = LogisticRegression(**parametros)\n",
    "\n",
    "    return modelo_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316ff230",
   "metadata": {},
   "source": [
    "#### **Instância Support Vector Classifier (SVC)**\n",
    "\n",
    "O SVC é um algoritimo que tem uma fundamentação matemática centrada em encontrar hiperplanos que maximizem as chamadas \"margens\" (distância entre o hiperplano de decisão e os exemplos mais próximos de cada classe) entre classes. Os hiperparâmetros controlam coisas como: o quão estrita a separação de pontos deve ser (hiperparâmetro c), que tipo de barreira de decisão usar (kernel) e o quão complexa essa barreira pode ser (gamma, degree). É vital otimizá-los para poder obter um modelo funcional que não superestime ou subestime a contribuição de uma variável.\n",
    "\n",
    "Hiperparâmetros Otimizados:\n",
    "\n",
    "- **``C``**: é um parâmetro regularizador, que controla a troca entre tamanho da margem e erro de classificação, um C pequeno tem melhor generalização, já um C maior tem um risco de _overfitting_.\n",
    "\n",
    "- **``kernel``**: controla o formato da fronteira de decisão.\n",
    "\n",
    "- **``gamma``**: é o coeficiente do kernel, que influencia o alcance de cada ponto individual no treinamento.\n",
    "\n",
    "- **``degree``**: é grau polinomial, caso o kernel seja polinomial, controla a complexidade do polinômio.\n",
    "\n",
    "- **``coef0``**: é o termo independente, que controla o deslocamento em kernels polinomiais ou sigmóides\n",
    "\n",
    "Além disso, há novamente o parâmetro `class_weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c66b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cria_instancia_svc(trial):\n",
    "    \"\"\"Cria a instância de um modelo de SVC\"\"\"\n",
    "    \n",
    "    parametros = {\n",
    "        \"C\": trial.suggest_float(\"C\", 0.01, 1000.0, log=True),\n",
    "        \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\",\"poly\", \"sigmoid\"]),\n",
    "        \"class_weight\": \"balanced\",\n",
    "        \"random_state\": SEED,\n",
    "        \"max_iter\": 600000,\n",
    "    }\n",
    "\n",
    "    if parametros[\"kernel\"] in [\"rbf\", \"poly\", \"sigmoid\"]:\n",
    "        entrada_gamma = trial.suggest_categorical(\"entrada_gamma\", [\"scale\", \"auto\", \"float\"])\n",
    "\n",
    "        if entrada_gamma == \"scale\":\n",
    "            parametros[\"gamma\"] = \"scale\"\n",
    "        elif entrada_gamma == \"auto\":\n",
    "            parametros[\"gamma\"] = \"auto\"\n",
    "        elif entrada_gamma == \"float\":\n",
    "            parametros[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-5, 1e2, log=True)\n",
    "    \n",
    "    if parametros[\"kernel\"] == \"poly\":\n",
    "        parametros[\"degree\"] = trial.suggest_int(\"degree\", 1, 4)\n",
    "\n",
    "    normalizar = trial.suggest_categorical(\"normalizar\", [True, False])\n",
    "\n",
    "    if normalizar:\n",
    "        tipo_normalizacao = trial.suggest_categorical(\"tipo_norm\", [\"Standard\", \"MinMax\", \"MaxAbs\"])\n",
    "\n",
    "        if tipo_normalizacao == \"Standard\":\n",
    "            normalizador = StandardScaler()\n",
    "        elif tipo_normalizacao == \"MinMax\":\n",
    "            normalizador = MinMaxScaler()\n",
    "        elif tipo_normalizacao == \"MaxAbs\":\n",
    "            normalizador = MaxAbsScaler()\n",
    "            \n",
    "        modelo_svc = make_pipeline(\n",
    "            normalizador,\n",
    "            SVC(**parametros)\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        modelo_svc = SVC(**parametros)\n",
    "\n",
    "    return modelo_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13b2a7",
   "metadata": {},
   "source": [
    "### **Função Objetivo**\n",
    "\n",
    "A seguir foi criada a função objetivo que é a função que irá computar a métrica de interesse. Neste caso, a métrica de interesse é a f1-macro, obtida por validação cruzada.\n",
    "\n",
    "A métrica ``f1-macro`` funciona da seguinte forma:\n",
    "\n",
    "- Precisão = Verdadeiros Positivos / (Verdadeiros Positivos + Falsos Positivos) \n",
    "    - De todas as previsões positivas, quantas eram realmente positivas\n",
    "\n",
    "- Recall = Verdadeiros Positivos / (Verdadeiros Positivos + Falsos Negativos) \n",
    "    - De todos os casos realmente positivos, quantos foram identificados corretamente\n",
    "\n",
    "A métrica calcula a média harmônica entre a precisão e o recall:\n",
    "\n",
    "- **F1 = 2 × (Precision × Recall) / (Precision + Recall)**\n",
    "\n",
    "Nela, cada rótulo do target tem o mesmo peso, independente de quantos exemplos tenha, o que é útil para o nosso caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f64401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcao_objetivo(trial, X, y, num_folds, modelo=\"knn\"):\n",
    "    \"\"\"Função objetivo do optuna\"\"\"\n",
    "\n",
    "    if modelo == \"knn\":\n",
    "        modelo = cria_instancia_knn(trial)\n",
    "    elif modelo == \"dtree\":\n",
    "        modelo = cria_instancia_dtree(trial)\n",
    "    elif modelo == \"rf\":\n",
    "        modelo = cria_instancia_rf(trial)\n",
    "    elif modelo == \"lr\":\n",
    "        modelo = cria_instancia_lr(trial)\n",
    "    elif modelo == \"svc\":\n",
    "        modelo = cria_instancia_svc(trial)\n",
    "    \n",
    "    metricas = cross_val_score(\n",
    "        modelo, \n",
    "        X, \n",
    "        y, \n",
    "        scoring=\"f1_macro\",\n",
    "        cv=num_folds,\n",
    "        )\n",
    "    \n",
    "    return metricas.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e9059",
   "metadata": {},
   "source": [
    "### **Otimizando os Hiperparâmetros**\n",
    "\n",
    "A seguir foram criados os estudos (conjunto de trials) usando o ``create_study()``, cujo argumento ``direction='maximize'`` tem como objetivo maximizar a f1-macro. Foi o utilizado o ``storage`` para armazenar o progreso da busca e o  ``load_if_exists`` para que seja possível continuar a busca de onde ela parou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d78199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOME_DO_ESTUDO_KNN = \"knn_nanotoxiclogia_optuna\"\n",
    "NOME_DO_ESTUDO_DTREE = \"dtree_nanotoxiclogia_optuna\"\n",
    "NOME_DO_ESTUDO_SVC = \"svc_nanotoxiclogia_optuna\"\n",
    "NOME_DO_ESTUDO_RF = \"rf_nanotoxiclogia_optuna\"\n",
    "NOME_DO_ESTUDO_LR = \"lr_nanotoxiclogia_optuna\"\n",
    "\n",
    "objeto_de_estudo_knn = create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=NOME_DO_ESTUDO_KNN,\n",
    "    storage=f\"sqlite:///../resultados_optuna/{NOME_DO_ESTUDO_KNN}.db\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "objeto_de_estudo_dtree = create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=NOME_DO_ESTUDO_DTREE,\n",
    "    storage=f\"sqlite:///../resultados_optuna/{NOME_DO_ESTUDO_DTREE}.db\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "objeto_de_estudo_svc = create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=NOME_DO_ESTUDO_SVC,\n",
    "    storage=f\"sqlite:///../resultados_optuna/{NOME_DO_ESTUDO_SVC}.db\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "objeto_de_estudo_rf = create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=NOME_DO_ESTUDO_RF,\n",
    "    storage=f\"sqlite:///../resultados_optuna/{NOME_DO_ESTUDO_RF}.db\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "objeto_de_estudo_lr = create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=NOME_DO_ESTUDO_LR,\n",
    "    storage=f\"sqlite:///../resultados_optuna/{NOME_DO_ESTUDO_LR}.db\",\n",
    "    load_if_exists=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf20fa",
   "metadata": {},
   "source": [
    "Para realmente rodar o otimizador precisamos de uma função objetivo que tenha apenas um argumento, o `trial`. Para isso vamos definir a `funcao_objetivo_parcial`.\n",
    "\n",
    "Serão usados ``modelo='modelo'`` para cada modelo e um número de fols na validação cruzada igual a 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27715019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def funcao_objetivo_parcial_knn(trial):\n",
    "    return funcao_objetivo(trial, X_treino, y_treino, NUM_FOLDS, modelo=\"knn\")\n",
    "\n",
    "def funcao_objetivo_parcial_dtree(trial):\n",
    "    return funcao_objetivo(trial, X_treino, y_treino, NUM_FOLDS, modelo=\"dtree\")\n",
    "\n",
    "def funcao_objetivo_parcial_rf(trial):\n",
    "    return funcao_objetivo(trial, X_treino, y_treino, NUM_FOLDS, modelo=\"rf\")\n",
    "\n",
    "def funcao_objetivo_parcial_svc(trial):\n",
    "    return funcao_objetivo(trial, X_treino, y_treino, NUM_FOLDS, modelo=\"svc\")\n",
    "\n",
    "def funcao_objetivo_parcial_lr(trial):\n",
    "    return funcao_objetivo(trial, X_treino, y_treino, NUM_FOLDS, modelo=\"lr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd3d69",
   "metadata": {},
   "source": [
    "Agora podemos definiro o número de novos trials e rodar cada otimização de modelo separadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3622265",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TENTATIVAS = 1\n",
    "objeto_de_estudo_knn.optimize(funcao_objetivo_parcial_knn, n_trials=NUM_TENTATIVAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TENTATIVAS = 1\n",
    "objeto_de_estudo_dtree.optimize(funcao_objetivo_parcial_dtree, n_trials=NUM_TENTATIVAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a559b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TENTATIVAS = 1\n",
    "objeto_de_estudo_svc.optimize(funcao_objetivo_parcial_svc, n_trials=NUM_TENTATIVAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TENTATIVAS = 1\n",
    "objeto_de_estudo_rf.optimize(funcao_objetivo_parcial_rf, n_trials=NUM_TENTATIVAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TENTATIVAS = 0\n",
    "objeto_de_estudo_lr.optimize(funcao_objetivo_parcial_lr, n_trials=NUM_TENTATIVAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736fff7d",
   "metadata": {},
   "source": [
    "### **Vizualizando os Resultados**\n",
    "\n",
    "Agora podemos analizar o foi obtido com cada modelo e a partir disso definir qual deles teve o melhor estimativa de resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7355bb",
   "metadata": {},
   "source": [
    "#### **Resultado Dummy (baseline):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A estimativa do f1-macro para o Dummy foi: {f1_macro_estimativa_dummy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1697cd",
   "metadata": {},
   "source": [
    "#### **Resultado K-NN:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4053b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "melhor_trial_knn = objeto_de_estudo_knn.best_trial\n",
    "\n",
    "print(f\"Número do melhor trial K-NN: {melhor_trial_knn.number}\")\n",
    "print(f\"Parâmetros do melhor trial : {melhor_trial_knn.params}\")\n",
    "print(f\"A melhor estimativa do f1-macro para o K-NN foi: {objeto_de_estudo_knn.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079f295",
   "metadata": {},
   "source": [
    "#### **Resultado Árvore de Decisão:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18191687",
   "metadata": {},
   "outputs": [],
   "source": [
    "melhor_trial_dtree = objeto_de_estudo_dtree.best_trial\n",
    "\n",
    "print(f\"Número do melhor trial da Árvore de Decisão: {melhor_trial_dtree.number}\")\n",
    "print(f\"Parâmetros do melhor trial da Árvore de Decisão: {melhor_trial_dtree.params}\")\n",
    "print(f\"A melhor estimativa do f1-macro para a Árvore de Decisão foi: {objeto_de_estudo_dtree.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde835d8",
   "metadata": {},
   "source": [
    "#### **Resultado SVC:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "melhor_trial_svc = objeto_de_estudo_svc.best_trial\n",
    "\n",
    "print(f\"Número do melhor trial do SVC: {melhor_trial_svc.number}\")\n",
    "print(f\"Parâmetros do melhor trial do SVC: {melhor_trial_svc.params}\")\n",
    "print(f\"A melhor estimativa do f1-macro para o SVC foi: {objeto_de_estudo_svc.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7deaa9c",
   "metadata": {},
   "source": [
    "#### **Resultado Floresta Aleatória:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf58a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "melhor_trial_rf = objeto_de_estudo_rf.best_trial\n",
    "\n",
    "print(f\"Número do melhor trial da Floresta Aleatória: {melhor_trial_rf.number}\")\n",
    "print(f\"Parâmetros do melhor trial da Floresta Aleatória: {melhor_trial_rf.params}\")\n",
    "print(f\"A melhor estimativa do f1-macro para a Floresta Aletória foi: {objeto_de_estudo_rf.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73877f5a",
   "metadata": {},
   "source": [
    "#### **Resultado Regressão Logística:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "melhor_trial_lr = objeto_de_estudo_lr.best_trial\n",
    "\n",
    "print(f\"Número do melhor trial da Regressão Logística: {melhor_trial_lr.number}\")\n",
    "print(f\"Parâmetros do melhor trial da Regressão Logística: {melhor_trial_lr.params}\")\n",
    "print(f\"A melhor estimativa do f1-macro para a Regressão Logística foi: {objeto_de_estudo_lr.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319dac1b",
   "metadata": {},
   "source": [
    "### **Conclusão sobre esse notebook**\n",
    "\n",
    "Neste notebook, implementamos e otimizamos diversos modelos de aprendizado de máquina para prever a toxicidade de nanopartículas com base em suas características físico-químicas e condições experimentais. Utilizamos técnicas como agrupamento de dados para evitar vazamento, codificação one-hot para variáveis categóricas e validação cruzada para garantir a qualidade das métricas.\n",
    "\n",
    "Os modelos foram otimizados com o Optuna, que permitiu encontrar combinações de hiperparâmetros que maximizassem a estimativa do F1-score macro.\n",
    "\n",
    "| Modelo               |   F1-Macro |   Melhor Trial |\n",
    "|----------------------|------------|----------------|\n",
    "| Baseline             |     0.4339 |              - |\n",
    "| K-NN                 |     0.8402 |            148 |\n",
    "| Árvore de Decisão    |     0.8961 |            781 |\n",
    "| SVC                  |     0.8508 |           1059 |\n",
    "| Floresta Aleatória   |     0.9112 |            898 |\n",
    "| Regressão Logística  |     0.7383 |             12 |\n",
    "\n",
    "A principio foi possível observar que:\n",
    "\n",
    "Todos os modelos superaram significativamente o baseline, mostrando que as features utilizadas possuem um valor preditivo. Além disso, modelos como Floresta Aleatória e a Árvore de Decisão tiveram um melhor resultado.\n",
    "\n",
    "O avaliação desses modelos nos dados de teste pode ser vista no arquivo **``notebooks\\avaliacao_modelos.ipynb``**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
